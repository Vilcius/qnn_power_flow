{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5d61c9e",
   "metadata": {},
   "source": [
    "# Quantum Neural Networks for Overload Detection and Power Flow Optimization\n",
    "\n",
    "## UTK REU 2025\n",
    "### Gianna Baker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c57f775",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"tennlogo.jpeg\", width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d254b652",
   "metadata": {},
   "source": [
    "# Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fd229a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandapower.networks as pn\n",
    "import pandapower as pp\n",
    "from pandapower.pypower.makeYbus import makeYbus\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as pnp\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd897dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b3808f",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26d8b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcOverload_from_net(net, capacities):\n",
    "    loading = net.res_line.loading_percent.values\n",
    "    return (loading > capacities).astype(int)\n",
    "\n",
    "# --- Helper to generate random P & Q (in per unit) ---\n",
    "def gen_signed_pq():\n",
    "    gen_P = np.random.uniform(0.5, 3.5)     # reduced upper bound\n",
    "    gen_Q = np.random.uniform(0.2, 2.0)\n",
    "\n",
    "    p2 = -np.random.uniform(2.0, 4.5)        # smaller loads\n",
    "    q2 = -np.random.uniform(0.5, 1.8)\n",
    "\n",
    "    p3 = -np.random.uniform(2.0, 4.5)\n",
    "    q3 = -np.random.uniform(0.5, 1.8)\n",
    "\n",
    "    return np.array([gen_P, gen_Q, p2, q2, p3, q3])\n",
    "\n",
    "# --- Create a new net and update bus injections ---\n",
    "def inject_and_run_pf(pq_vector, capacities = 100):\n",
    "    gen_P, gen_Q, p2, q2, p3, q3 = pq_vector\n",
    "    net = pn.case4gs()\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Scale back to MW/MVar assuming 100 baseMVA\n",
    "        net.gen.at[0, 'p_mw'] = gen_P * 100\n",
    "        net.gen.at[0, 'q_mvar'] = gen_Q * 100\n",
    "\n",
    "        net.load.at[0, 'p_mw'] = -p2 * 100\n",
    "        net.load.at[0, 'q_mvar'] = -q2 * 100\n",
    "        net.load.at[1, 'p_mw'] = -p3 * 100\n",
    "        net.load.at[1, 'q_mvar'] = -q3 * 100\n",
    "\n",
    "        # Optional: push line capacity limits lower to provoke overloads\n",
    "        net.line['max_loading_percent'] = capacities\n",
    "\n",
    "        pp.runpp(net)\n",
    "        #print(net.res_line.loading_percent)\n",
    "\n",
    "        result = calcOverload_from_net(net, capacities)\n",
    "\n",
    "        return result\n",
    "\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Main loop for pattern matching ---\n",
    "all_patterns = list(product([0, 1], repeat=4))\n",
    "pattern_weights = {\n",
    "    (0, 0, 0, 0): 1.3,\n",
    "    (0, 0, 0, 1): 1.3,\n",
    "    (0, 0, 1, 0): 1.5,\n",
    "    (0, 0, 1, 1): 2.0,  # lower accuracy → more emphasis\n",
    "    (0, 1, 0, 0): 1.0,\n",
    "    (0, 1, 0, 1): 0.8,\n",
    "    (0, 1, 1, 0): 1.3,\n",
    "    (0, 1, 1, 1): 2.0,\n",
    "    (1, 0, 0, 0): 1.0,\n",
    "    (1, 0, 0, 1): 1.7,\n",
    "    (1, 0, 1, 0): 1.1,\n",
    "    (1, 0, 1, 1): 2.0,\n",
    "    (1, 1, 0, 0): 1.0,\n",
    "    (1, 1, 0, 1): 1.2,\n",
    "    (1, 1, 1, 0): 2.0,\n",
    "    (1, 1, 1, 1): 1.3,\n",
    "} # Use your preferred weight dict here\n",
    "\n",
    "base_target = 100\n",
    "X = np.load('/content/drive/My Drive/XFINAL.npy').tolist()\n",
    "Y = np.load('/content/drive/My Drive/YFINAL.npy').tolist()\n",
    "Y_tuples = [tuple(y) for y in Y]\n",
    "existing_counts = Counter(Y_tuples)\n",
    "\n",
    "for target in all_patterns:\n",
    "    target_np = np.array(target)\n",
    "    count = existing_counts.get(target, 0)  # START with already existing\n",
    "    tries = 0\n",
    "    adjusted_target = int(base_target * pattern_weights.get(target, 1.0))\n",
    "    adjusted_target = max(adjusted_target, 10)\n",
    "\n",
    "    print(f\"🎯 Target pattern {target} → samples: {adjusted_target}\")\n",
    "\n",
    "    while count < adjusted_target and tries < 1000:\n",
    "        pq_vec = gen_signed_pq()\n",
    "        result = inject_and_run_pf(pq_vec, [70,72,63,36])\n",
    "\n",
    "        if result is None:\n",
    "            tries += 1\n",
    "            continue\n",
    "\n",
    "\n",
    "        #print(result)\n",
    "        if np.array_equal(result, target_np):\n",
    "            X.append(pq_vec)\n",
    "            Y.append(result)\n",
    "            count += 1\n",
    "            print(f\"  ✔ Match {count}/{adjusted_target} → {pq_vec}\")\n",
    "\n",
    "        tries += 1\n",
    "\n",
    "    if count < 10:\n",
    "        print(f\"⚠️ Only found {count}/10 for pattern {target} after {tries} tries.\")\n",
    "\n",
    "print(\"✅ Data generation complete!\")\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"Y shape:\", Y.shape)\n",
    "np.save('/content/drive/MyDrive/X.npy', X)\n",
    "np.save('/content/drive/MyDrive/Y.npy', Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546a04c6",
   "metadata": {},
   "source": [
    "### If all 16 patterns not found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4debdede",
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_patterns = set(tuple(y) for y in Y)  # Y is your existing label array\n",
    "\n",
    "all_patterns = list(product([0, 1], repeat=4))\n",
    "\n",
    "while len(seen_patterns) < 16:\n",
    "    pq_vec = gen_signed_pq()\n",
    "    overload = inject_and_run_pf(pq_vec, capacities=[70,72,63,36]) # 69 68 63 36\n",
    "\n",
    "    if overload is None:\n",
    "        continue\n",
    "    #print(overload)\n",
    "\n",
    "    pattern = tuple(overload)\n",
    "    if pattern not in seen_patterns:\n",
    "        seen_patterns.add(pattern)\n",
    "        X.append(pq_vec)\n",
    "        Y.append(overload)\n",
    "        print(f\"✔ Found new pattern {pattern} ({len(seen_patterns)}/16)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a1764e",
   "metadata": {},
   "source": [
    "#### Double Check Shape (sanity check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcc384d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:-2]\n",
    "Y = Y[:-2]\n",
    "\n",
    "X = X[:, -6:]\n",
    "print(np.shape(X))\n",
    "print(np.shape(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c305f6f",
   "metadata": {},
   "source": [
    "### Balance Data if Harsh Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c400b5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Y rows to tuple keys for grouping\n",
    "Y_tuples = [tuple(y) for y in Y]\n",
    "\n",
    "# Group the Xs and Ys by label\n",
    "grouped = defaultdict(list)\n",
    "for x, y in zip(X, Y_tuples):\n",
    "    grouped[y].append(x)\n",
    "\n",
    "# Get max group size\n",
    "max_len = max(len(v) for v in grouped.values())\n",
    "\n",
    "# Oversample each group to match max size\n",
    "X_balanced = []\n",
    "Y_balanced = []\n",
    "\n",
    "for label, x_group in grouped.items():\n",
    "    x_array = np.array(x_group)\n",
    "    needed = max_len - len(x_array)\n",
    "\n",
    "    # Sample with replacement to pad group\n",
    "    if needed > 0:\n",
    "        extra = x_array[np.random.choice(len(x_array), needed, replace=True)]\n",
    "        x_full = np.vstack([x_array, extra])\n",
    "    else:\n",
    "        x_full = x_array\n",
    "\n",
    "    y_full = [np.array(label)] * len(x_full)\n",
    "\n",
    "    X_balanced.append(x_full)\n",
    "    Y_balanced.extend(y_full)\n",
    "\n",
    "# Concatenate all groups together\n",
    "X = np.vstack(X_balanced)\n",
    "Y = np.array(Y_balanced)\n",
    "\n",
    "# Optional: Shuffle before scaling\n",
    "shuffle_indices = np.random.permutation(len(X))\n",
    "X = X[shuffle_indices]\n",
    "Y = Y[shuffle_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82726e9b",
   "metadata": {},
   "source": [
    "## Change Size of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89bef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_balanced_subset(X, Y, samples_per_pattern=20):\n",
    "    pattern_dict = defaultdict(list)\n",
    "\n",
    "    # Group samples by unique overload pattern\n",
    "    for idx, y in enumerate(Y):\n",
    "        pattern_key = tuple(y)  # make it hashable\n",
    "        pattern_dict[pattern_key].append(idx)\n",
    "\n",
    "    selected_indices = []\n",
    "    for indices in pattern_dict.values():\n",
    "        if len(indices) > samples_per_pattern:\n",
    "            chosen = np.random.choice(indices, samples_per_pattern, replace=False)\n",
    "        else:\n",
    "            chosen = indices  # Keep all if fewer than requested\n",
    "        selected_indices.extend(chosen)\n",
    "\n",
    "    # Convert to arrays\n",
    "    X_selected = np.array(X)[selected_indices]\n",
    "    Y_selected = np.array(Y)[selected_indices]\n",
    "    return X_selected, Y_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b11a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change number for how many samples per overload pattern\n",
    "X, Y = select_balanced_subset(X,Y,70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4938a4",
   "metadata": {},
   "source": [
    "# Scale Data, Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9c2cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=42)\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "Y_train = torch.tensor(Y_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "Y_test = torch.tensor(Y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54aaad9",
   "metadata": {},
   "source": [
    "# Overload Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2656f14",
   "metadata": {},
   "source": [
    "## Classical Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ded915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same Scale Network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(6, 10), # Input size is 8 (P and Q for 4 buses)\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.2),\n",
    "            nn.Linear(10, 10),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.2),\n",
    "            nn.Linear(10, 10),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.2),\n",
    "            nn.Linear(10, 4),# Output size is 4 (for the 4 overload flags)\n",
    "            nn.Sigmoid()  # for multilabel classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "model = SimpleNN()\n",
    "\n",
    "# === Loss and optimizer ===\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# === Training loop ===\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, Y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == num_epochs - 1:\n",
    "        print(f\"Epoch {epoch} | Avg Loss: {loss.item():.6f}\")\n",
    "\n",
    "# === Evaluation ===\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(X_test)\n",
    "    preds_bin = (preds >= 0.5).int()\n",
    "    correct = (preds_bin == Y_test.int()).all(dim=1).sum().item()\n",
    "    total = Y_test.shape[0]\n",
    "\n",
    "    print(\"\\n--- Testing on unseen data ---\")\n",
    "    print(f\"Accuracy: {correct} / {total} ({100 * correct / total:.2f}%)\")\n",
    "    loss_test = criterion(preds, Y_test).item()\n",
    "    print(f\"Average Test Loss (BCE): {loss_test:.6f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    print(\"\\nSample predictions:\")\n",
    "    for i in range(10):\n",
    "        print(f\"Input:     {X_test[i].numpy()}\")\n",
    "        print(f\"True Y:    {Y_test[i].numpy().astype(int)}\")\n",
    "        print(f\"Predicted: {preds_bin[i].numpy()}\")\n",
    "        print(\"-\" * 40)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Convert predictions and ground truth to NumPy arrays\n",
    "Y_pred_all = preds.numpy()\n",
    "Y_pred_bin = (Y_pred_all >= 0.5).astype(int)\n",
    "Y_true_all = Y_test.numpy().astype(int)\n",
    "\n",
    "# Accuracy per output bit\n",
    "bit_acc = (Y_pred_bin == Y_true_all).sum(axis=0) / Y_true_all.shape[0]\n",
    "for i, acc in enumerate(bit_acc):\n",
    "    print(f\"Accuracy for output bit {i} (Line {i+1}): {acc:.2%}\")\n",
    "\n",
    "# Optional: Confusion matrix per output bit\n",
    "for i in range(4):\n",
    "    print(f\"\\nConfusion matrix for bit {i} (Line {i+1}):\")\n",
    "    print(confusion_matrix(Y_true_all[:, i], Y_pred_bin[:, i]))\n",
    "pred_cases = [tuple(row) for row in Y_pred_bin]\n",
    "true_cases = [tuple(row) for row in Y_true_all]\n",
    "\n",
    "# Count how many times each pattern was predicted\n",
    "pred_counter = Counter(pred_cases)\n",
    "true_counter = Counter(true_cases)\n",
    "\n",
    "# Get all unique patterns (present in either true or predicted)\n",
    "all_patterns = sorted(set(pred_counter.keys()).union(set(true_counter.keys())))\n",
    "\n",
    "print(\"\\n--- Pattern Breakdown ---\")\n",
    "print(f\"{'Pattern':<10} | {'True Count':>10} | {'Pred Count':>10}\")\n",
    "print(\"-\" * 36)\n",
    "for pattern in all_patterns:\n",
    "    print(f\"{list(pattern)!s:<10} | {true_counter.get(pattern, 0):>10} | {pred_counter.get(pattern, 0):>10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224847d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Large Network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(6, 148), # Input size is 8 (P and Q for 4 buses)\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(148, 84),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(84, 24),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(24, 4), # Output size is 4 (for the 4 overload flags)\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "model = SimpleNN()\n",
    "\n",
    "# === Loss and optimizer ===\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# === Training loop ===\n",
    "num_epochs = 600\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, Y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == num_epochs - 1:\n",
    "        print(f\"Epoch {epoch} | Avg Loss: {loss.item():.6f}\")\n",
    "\n",
    "# === Evaluation ===\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(X_test)\n",
    "    preds = torch.sigmoid(logits)\n",
    "    preds_bin = (preds >= 0.5).int()\n",
    "    correct = (preds_bin == Y_test.int()).all(dim=1).sum().item()\n",
    "    total = Y_test.shape[0]\n",
    "\n",
    "    print(\"\\n--- Testing on unseen data ---\")\n",
    "    print(f\"Accuracy: {correct} / {total} ({100 * correct / total:.2f}%)\")\n",
    "    loss_test = criterion(preds, Y_test).item()\n",
    "    print(f\"Average Test Loss (BCE): {loss_test:.6f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    print(\"\\nSample predictions:\")\n",
    "    for i in range(10):\n",
    "        print(f\"Input:     {X_test[i].numpy()}\")\n",
    "        print(f\"True Y:    {Y_test[i].numpy().astype(int)}\")\n",
    "        print(f\"Predicted: {preds_bin[i].numpy()}\")\n",
    "        print(\"-\" * 40)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Convert predictions and ground truth to NumPy arrays\n",
    "Y_pred_all = preds.numpy()\n",
    "Y_pred_bin = (Y_pred_all >= 0.5).astype(int)\n",
    "Y_true_all = Y_test.numpy().astype(int)\n",
    "\n",
    "# Accuracy per output bit\n",
    "bit_acc = (Y_pred_bin == Y_true_all).sum(axis=0) / Y_true_all.shape[0]\n",
    "for i, acc in enumerate(bit_acc):\n",
    "    print(f\"Accuracy for output bit {i} (Line {i+1}): {acc:.2%}\")\n",
    "\n",
    "# Optional: Confusion matrix per output bit\n",
    "for i in range(4):\n",
    "    print(f\"\\nConfusion matrix for bit {i} (Line {i+1}):\")\n",
    "    print(confusion_matrix(Y_true_all[:, i], Y_pred_bin[:, i]))\n",
    "pred_cases = [tuple(row) for row in Y_pred_bin]\n",
    "true_cases = [tuple(row) for row in Y_true_all]\n",
    "\n",
    "# Count how many times each pattern was predicted\n",
    "pred_counter = Counter(pred_cases)\n",
    "true_counter = Counter(true_cases)\n",
    "\n",
    "# Get all unique patterns (present in either true or predicted)\n",
    "all_patterns = sorted(set(pred_counter.keys()).union(set(true_counter.keys())))\n",
    "\n",
    "print(\"\\n--- Pattern Breakdown ---\")\n",
    "print(f\"{'Pattern':<10} | {'True Count':>10} | {'Pred Count':>10}\")\n",
    "print(\"-\" * 36)\n",
    "for pattern in all_patterns:\n",
    "    print(f\"{list(pattern)!s:<10} | {true_counter.get(pattern, 0):>10} | {pred_counter.get(pattern, 0):>10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef24cdb",
   "metadata": {},
   "source": [
    "## Quantum Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aa3dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(TensorDataset(X_train_tensor, Y_train_tensor), batch_size=16, shuffle=True)\n",
    "\n",
    "# ==== Quantum Circuit ====\n",
    "n_qubits = 10\n",
    "n_layers = 3\n",
    "n_features = 6\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\", diff_method=\"backprop\")\n",
    "def qnode(inputs, weights):\n",
    "    for i in range(n_features):\n",
    "        qml.Hadamard(wires=i)\n",
    "    qml.AngleEmbedding(inputs, wires=range(n_features), rotation='Z')\n",
    "\n",
    "\n",
    "    qml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
    "\n",
    "    # Custom entanglement\n",
    "    qml.CNOT(wires=[0,1])\n",
    "    qml.CNOT(wires=[2,3])\n",
    "    qml.CNOT(wires=[4,5])\n",
    "    qml.CNOT(wires=[6,7])\n",
    "    qml.CNOT(wires=[0,4])\n",
    "    qml.CNOT(wires=[1,5])\n",
    "    qml.CNOT(wires=[0,2])\n",
    "    qml.CNOT(wires=[1,3])\n",
    "    qml.CNOT(wires=[2,6])\n",
    "    qml.CNOT(wires=[3,7])\n",
    "    qml.CNOT(wires=[4,6])\n",
    "    qml.CNOT(wires=[5,7])\n",
    "\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(n_features)]\n",
    "\n",
    "# ==== QNN Model Class ====\n",
    "class QNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.q_layer = qml.qnn.TorchLayer(qnode, {\"weights\": (n_layers, n_qubits, 3)})\n",
    "        self.linear = nn.Linear(n_features, 4)  # Reduce qubits → 4 outputs\n",
    "        self.tune = nn.Parameter(torch.tensor(1.0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.q_layer(x)  # shape: (batch_size, 8)\n",
    "        z = self.linear(z)   # shape: (batch_size, 4)\n",
    "        self.tune = nn.Parameter(torch.tensor(5.0))\n",
    "        return self.tune * z  # shape: (batch_size, 4)\n",
    "\n",
    "\n",
    "\n",
    "# ==== Training ====\n",
    "model = QNNModel()\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# ==== Training Loop ====\n",
    "n_epochs = 100\n",
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(x_batch)\n",
    "        loss = loss_fn(preds, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1:02d} | Avg Loss: {total_loss / len(train_loader):.6f}\")\n",
    "\n",
    "\n",
    "# ==== Evaluation ====\n",
    "with torch.no_grad():\n",
    "    logits = model(X_test_tensor)\n",
    "    preds = torch.sigmoid(logits)\n",
    "    bin_preds = (preds > 0.5).int()\n",
    "    correct = (bin_preds == Y_test_tensor.int()).all(dim=1).sum().item()\n",
    "    print(f\"\\nExact Match Accuracy: {correct} / {len(Y_test_tensor)} = {100 * correct / len(Y_test_tensor):.2f}%\")\n",
    "\n",
    "    print(\"\\nSample Predictions:\")\n",
    "    for i in range(3):\n",
    "        x = X_test_tensor[i]\n",
    "        y_true = Y_test_tensor[i]\n",
    "        y_pred = model(x.unsqueeze(0)).squeeze().detach().numpy()\n",
    "        y_bin = (y_pred > 0.5).astype(int)\n",
    "        print(f\"Input:     {x.numpy()}\")\n",
    "        print(f\"True Y:    {y_true.numpy()}\")\n",
    "        print(f\"Predicted: {y_bin}\")\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1df2b4",
   "metadata": {},
   "source": [
    "# Accuracy Breakdown Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66d3e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this to see the accuracy per pattern\n",
    "# Convert tensors to numpy arrays\n",
    "y_true_np = Y_test_tensor.int().numpy()\n",
    "y_pred_np = bin_preds.numpy()\n",
    "\n",
    "# Create pattern strings for true and predicted labels for easy comparison\n",
    "true_patterns = [\"\".join(map(str, row)) for row in y_true_np]\n",
    "pred_patterns = [\"\".join(map(str, row)) for row in y_pred_np]\n",
    "\n",
    "# Get all unique true patterns in test set\n",
    "unique_patterns = set(true_patterns)\n",
    "\n",
    "# Store counts per pattern\n",
    "pattern_counts = defaultdict(int)\n",
    "pattern_correct = defaultdict(int)\n",
    "\n",
    "# Loop through all samples\n",
    "for true_patt, pred_patt in zip(true_patterns, pred_patterns):\n",
    "    pattern_counts[true_patt] += 1\n",
    "    if true_patt == pred_patt:\n",
    "        pattern_correct[true_patt] += 1\n",
    "\n",
    "# Print accuracy per pattern\n",
    "for patt in sorted(unique_patterns):\n",
    "    count = pattern_counts[patt]\n",
    "    correct = pattern_correct[patt]\n",
    "    accuracy = correct / count if count > 0 else 0\n",
    "    print(f\"Pattern {patt}: Accuracy {accuracy:.2%} ({correct}/{count})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a1cfaa",
   "metadata": {},
   "source": [
    "# Correction Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c6bd23",
   "metadata": {},
   "source": [
    "## Generate y-true values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50ad006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(inputVector, capacities=None):\n",
    "    step_size = 0.01\n",
    "    gen_P, gen_Q, p2, q2, p3, q3 = inputVector\n",
    "    delta_P_range = np.linspace(-5.0, 5.0, 70)  # Try P changes from -5 to +5\n",
    "    delta_Q_range = np.linspace(-5.0, 5.0, 70)\n",
    "\n",
    "    for dP in delta_P_range:\n",
    "      for dQ in delta_Q_range:\n",
    "            if dP == 0 and dQ == 0:\n",
    "                continue  # Skip the original point\n",
    "\n",
    "            trial_P = gen_P + dP\n",
    "            trial_Q = gen_Q + dQ\n",
    "            vdelta_vec = np.empty(6)\n",
    "\n",
    "            if trial_P <= 0 or trial_Q <= 0:\n",
    "              continue\n",
    "\n",
    "            net = pn.case4gs()\n",
    "\n",
    "\n",
    "            try:\n",
    "                # Solve power flow with new generator settings\n",
    "                # Update generator P & Q\n",
    "              net.gen.at[0, 'p_mw'] = trial_P * 100\n",
    "              net.gen.at[0, 'q_mvar'] = trial_Q * 100\n",
    "\n",
    "              if capacities is not None:\n",
    "              # Set tight line loading limits\n",
    "                net.line['max_loading_percent'] = capacities\n",
    "\n",
    "\n",
    "\n",
    "              # Update loads\n",
    "              net.load.at[0, 'p_mw'] = -p2 * 100\n",
    "              net.load.at[0, 'q_mvar'] = -q2 * 100\n",
    "              net.load.at[1, 'p_mw'] = -p3 * 100\n",
    "              net.load.at[1, 'q_mvar'] = -q3 * 100\n",
    "\n",
    "              pp.runpp(net)\n",
    "              ppc = net._ppc\n",
    "              #Ybus, _, _ = makeYbus(ppc[\"baseMVA\"], ppc[\"bus\"], ppc[\"branch\"])\n",
    "\n",
    "              # Pull voltage magnitudes and angles\n",
    "              V = net.res_bus.vm_pu.values\n",
    "              delta = np.deg2rad(net.res_bus.va_degree.values)\n",
    "\n",
    "              # Strip slack bus (bus 0) and interleave V & δ\n",
    "              vdelta_vec = np.array([v for pair in zip(V[1:], delta[1:]) for v in pair])\n",
    "\n",
    "\n",
    "\n",
    "            except:\n",
    "                print(\"Didnt work\")\n",
    "                continue  # Skip if power flow doesn't converge\n",
    "\n",
    "            loading = net.res_line.loading_percent.values\n",
    "            overload_flags = (loading > capacities).astype(int)\n",
    "            if np.all(overload_flags == 0):\n",
    "\n",
    "                output_vector = [dP, dQ]\n",
    "\n",
    "                return output_vector\n",
    "\n",
    "    return None  # No fix found\n",
    "\n",
    "X_fix = []\n",
    "Y_fix = []\n",
    "\n",
    "for x, y in zip(X,Y):\n",
    "\n",
    "    # Skip samples that aren't overloaded to begin with\n",
    "    if all(y == 0):\n",
    "      print(\"Skip\")\n",
    "      continue\n",
    "\n",
    "    else:\n",
    "      result = correct(x, [70,72,63,36])\n",
    "\n",
    "      if result:\n",
    "          print(x)\n",
    "          X_fix.append(x)\n",
    "          Y_fix.append(result)\n",
    "np.save('/content/drive/MyDrive/Xfix.npy', X_fix)\n",
    "np.save('/content/drive/MyDrive/Yfix.npy', Y_fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89403f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale for networks\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_fix)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y_fix, test_size=0.2, random_state=42)\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "Y_train = torch.tensor(Y_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "Y_test = torch.tensor(Y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7083c512",
   "metadata": {},
   "source": [
    "## Classical Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327e15f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Define the model ===\n",
    "class SmallNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(6, 10), # Input size is 8 (P and Q for 4 buses)\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.2),\n",
    "            nn.Linear(10, 10),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.2),\n",
    "            nn.Linear(10, 10),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.2),\n",
    "            nn.Linear(10, 2),# Output size is 4 (for the 4 overload flags)\n",
    "            nn.Sigmoid()  # for multilabel classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "model = SmallNN()\n",
    "\n",
    "# === Loss and optimizer ===\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# === Training loop ===\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, Y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == num_epochs - 1:\n",
    "        print(f\"Epoch {epoch} | Avg Loss: {loss.item():.6f}\")\n",
    "\n",
    "# === Evaluation ===\n",
    "correct = 0\n",
    "total = len(X_test)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(X_test)  # Shape: (N, 2)\n",
    "\n",
    "for i in range(total):\n",
    "    # Step 1: Modify the original X vector with the predicted values\n",
    "    x_input = X_test[i].clone().numpy()\n",
    "    x_input = scaler.inverse_transform(x_input.reshape(1, -1)).flatten()\n",
    "\n",
    "    p_pred, q_pred = preds[i].numpy()\n",
    "\n",
    "    # Insert predicted values into the first two positions (e.g., P2, Q2)\n",
    "    x_input[0] += p_pred\n",
    "    x_input[1] += q_pred\n",
    "    fixed_vec = x_input\n",
    "\n",
    "    # Step 2: Reconstruct your net (or copy it fresh if you have a template)\n",
    "    net = pn.case4gs()\n",
    "\n",
    "    net.gen.at[0, 'p_mw'] = fixed_vec[0] * 100\n",
    "    net.gen.at[0, 'q_mvar'] = fixed_vec[1] * 100\n",
    "    net.load.at[0, 'p_mw'] = -fixed_vec[2] * 100\n",
    "    net.load.at[0, 'q_mvar'] = -fixed_vec[3] * 100\n",
    "    net.load.at[1, 'p_mw'] = -fixed_vec[4] * 100\n",
    "    net.load.at[1, 'q_mvar'] = -fixed_vec[5] * 100\n",
    "    net.line['max_loading_percent'] = [70,72,63,36]\n",
    "\n",
    "    # Step 4: Run power flow\n",
    "    try:\n",
    "        pp.runpp(net)\n",
    "        line_loading = net.res_line.loading_percent.values\n",
    "        overload_flags = line_loading > 100.0\n",
    "\n",
    "        if not np.any(overload_flags):\n",
    "            correct += 1\n",
    "\n",
    "    except pp.LoadflowNotConverged:\n",
    "        print(f\"Power flow did not converge for sample {i}\")\n",
    "\n",
    "print(f\"\\nPost-simulation accuracy (no overloads after predicted fix): {correct} / {total} ({100 * correct / total:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcc1d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Large classical network\n",
    "# === Define the model ===\n",
    "class CorrectionNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(6, 148), # Input size is 8 (P and Q for 4 buses)\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(148, 84),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(84, 24),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(24, 2), # Output size is 4 (for the 4 overload flags)\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "model = CorrectionNN()\n",
    "\n",
    "# === Loss and optimizer ===\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# === Training loop ===\n",
    "num_epochs = 600\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, Y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == num_epochs - 1:\n",
    "        print(f\"Epoch {epoch} | Avg Loss: {loss.item():.6f}\")\n",
    "\n",
    "# === Evaluation ===\n",
    "correct = 0\n",
    "total = len(X_test)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(X_test)  # Shape: (N, 2)\n",
    "\n",
    "for i in range(total):\n",
    "    # Step 1: Modify the original X vector with the predicted values\n",
    "    x_input = X_test[i].clone().numpy()\n",
    "    x_input = scaler.inverse_transform(x_input.reshape(1, -1)).flatten()\n",
    "\n",
    "    p_pred, q_pred = preds[i].numpy()\n",
    "\n",
    "    # Insert predicted values into the first two positions (e.g., P2, Q2)\n",
    "    x_input[0] += p_pred\n",
    "    x_input[1] += q_pred\n",
    "    fixed_vec = x_input\n",
    "\n",
    "    # Step 2: Reconstruct your net (or copy it fresh if you have a template)\n",
    "    net = pn.case4gs()\n",
    "\n",
    "    net.gen.at[0, 'p_mw'] = fixed_vec[0] * 100\n",
    "    net.gen.at[0, 'q_mvar'] = fixed_vec[1] * 100\n",
    "    net.load.at[0, 'p_mw'] = -fixed_vec[2] * 100\n",
    "    net.load.at[0, 'q_mvar'] = -fixed_vec[3] * 100\n",
    "    net.load.at[1, 'p_mw'] = -fixed_vec[4] * 100\n",
    "    net.load.at[1, 'q_mvar'] = -fixed_vec[5] * 100\n",
    "    net.line['max_loading_percent'] = [70,72,63,36]\n",
    "\n",
    "    # Step 4: Run power flow\n",
    "    try:\n",
    "        pp.runpp(net)\n",
    "        line_loading = net.res_line.loading_percent.values\n",
    "        overload_flags = line_loading > 100.0\n",
    "\n",
    "        if not np.any(overload_flags):\n",
    "            correct += 1\n",
    "\n",
    "    except pp.LoadflowNotConverged:\n",
    "        print(f\"Power flow did not converge for sample {i}\")\n",
    "\n",
    "print(f\"\\nPost-simulation accuracy (no overloads after predicted fix): {correct} / {total} ({100 * correct / total:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2008552",
   "metadata": {},
   "source": [
    "## Quantum Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1479b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Quantum Circuit ====\n",
    "n_qubits = 10\n",
    "n_layers = 3\n",
    "n_features = 6\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\", diff_method=\"backprop\")\n",
    "def qnode(inputs, weights):\n",
    "    for i in range(n_features):\n",
    "        qml.Hadamard(wires=i)\n",
    "    qml.AngleEmbedding(inputs, wires=range(n_features), rotation='Z')\n",
    "\n",
    "\n",
    "    qml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
    "\n",
    "    # Custom entanglement\n",
    "    qml.CNOT(wires=[0,1])\n",
    "    qml.CNOT(wires=[2,3])\n",
    "    qml.CNOT(wires=[4,5])\n",
    "    qml.CNOT(wires=[6,7])\n",
    "    qml.CNOT(wires=[6,0])\n",
    "    qml.CNOT(wires=[7,1])\n",
    "    qml.CNOT(wires=[6,2])\n",
    "    qml.CNOT(wires=[7,3])\n",
    "\n",
    "    qml.CNOT(wires=[0,4])\n",
    "    qml.CNOT(wires=[1,5])\n",
    "    qml.CNOT(wires=[2,4])\n",
    "    qml.CNOT(wires=[3,5])\n",
    "\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(n_features)]\n",
    "\n",
    "# ==== QNN Model Class ====\n",
    "class QNNFix(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.q_layer = qml.qnn.TorchLayer(qnode, {\"weights\": (n_layers, n_qubits, 3)})\n",
    "        self.linear = nn.Linear(n_features, 2)  # Reduce qubits → 2 outputs\n",
    "        self.tune = nn.Parameter(torch.tensor(1.0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.q_layer(x)\n",
    "        z = self.linear(z)   \n",
    "        return self.tune * z  \n",
    "\n",
    "\n",
    "\n",
    "# ==== Training ====\n",
    "model = QNNFix()\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# ==== Training Loop ====\n",
    "n_epochs = 100\n",
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(x_batch)\n",
    "        loss = loss_fn(preds, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1:02d} | Avg Loss: {total_loss / len(train_loader):.6f}\")\n",
    "\n",
    "\n",
    "# ==== Evaluation ====\n",
    "correct = 0\n",
    "total = len(X_test_tensor)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(X_test_tensor).cpu().numpy()  # shape: (N, 2)\n",
    "\n",
    "for i in range(total):\n",
    "    x_input = X_test_tensor[i].numpy()\n",
    "    x_input_unscaled = scaler.inverse_transform(x_input.reshape(1, -1)).flatten()\n",
    "\n",
    "    # Get predicted correction\n",
    "    p_pred, q_pred = preds[i]\n",
    "\n",
    "    # Modify P2 and Q2\n",
    "    x_input_unscaled[0] += p_pred  # P2\n",
    "    x_input_unscaled[1] += q_pred  # Q2\n",
    "    fixed_vec = x_input_unscaled\n",
    "\n",
    "    # Rebuild your base network (e.g., 4-bus)\n",
    "    net = pn.case4gs()\n",
    "    net.gen.at[0, 'p_mw'] = fixed_vec[0] * 100\n",
    "    net.gen.at[0, 'q_mvar'] = fixed_vec[1] * 100\n",
    "    net.load.at[0, 'p_mw'] = -fixed_vec[2] * 100\n",
    "    net.load.at[0, 'q_mvar'] = -fixed_vec[3] * 100\n",
    "    net.load.at[1, 'p_mw'] = -fixed_vec[4] * 100\n",
    "    net.load.at[1, 'q_mvar'] = -fixed_vec[5] * 100\n",
    "    net.line['max_loading_percent'] = [70, 72, 63, 36]\n",
    "\n",
    "    try:\n",
    "        pp.runpp(net)\n",
    "        line_loading = net.res_line.loading_percent.values\n",
    "        overload_flags = line_loading > 100.0\n",
    "\n",
    "        if not np.any(overload_flags):\n",
    "            correct += 1\n",
    "\n",
    "    except pp.LoadflowNotConverged:\n",
    "        print(f\"Quantum power flow failed for sample {i}\")\n",
    "\n",
    "print(f\"\\nQuantum Post-fix Accuracy (no overloads): {correct} / {total} ({100 * correct / total:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babbf210",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
